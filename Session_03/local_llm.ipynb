{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from ollama) (2.12.3)\n",
      "Requirement already satisfied: anyio in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from httpx>=0.27->ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kannans100/sansa/.venv/lib/python3.13/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Downloading ollama-0.6.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#MacOS / Linux\n",
    "# python -m venv .venv\n",
    "# source .venv/bin/activate\n",
    "\n",
    "# Windows PowerShell\n",
    "# python -m venv .venv\n",
    "# .venv\\Scripts\\Activate.ps1\n",
    "\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def ask_question_local_llm(prompt):\n",
    "\n",
    "    print(f\"User asked: {prompt}\")\n",
    "\n",
    "    # Run a prompt against a local model (e.g., llama3)\n",
    "    response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assitant - Respond in one line\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User asked: How old is Python?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Python was created in 1991 by Guido van Rossum, and its first version, Python 0.9.1, was released in February 1994, making it approximately 32 years old as of 2023.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ask_question_local_llm(\"How old is Python?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: what is my name\n",
      "Time taken by Local LLM: 1.6183459758758545 seconds\n",
      "\n",
      "Local LLM says: I apologize, but I don't have any information about your name. Can you please tell me what it is?\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: how to learn chat gpt\n",
      "Time taken by Local LLM: 2.820189952850342 seconds\n",
      "\n",
      "Local LLM says: To learn about ChatGPT, start by reading its documentation and understanding its capabilities, then practice using it to generate text on various topics, experiment with different prompts and formats, and analyze its outputs to improve your own writing style.\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: \n",
      "Time taken by Local LLM: 1.3769888877868652 seconds\n",
      "\n",
      "Local LLM says: I'm here to help, what's on your mind and how can I assist you today?\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: \n",
      "Time taken by Local LLM: 0.6089086532592773 seconds\n",
      "\n",
      "Local LLM says: How can I assist you today?\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: \n",
      "Time taken by Local LLM: 0.6020059585571289 seconds\n",
      "\n",
      "Local LLM says: How can I assist you today?\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "while True:\n",
    "    # Ask user for a question\n",
    "    user_prompt = input(\"Ask something: \")\n",
    "\n",
    "    if (user_prompt.lower() != 'quit'):\n",
    "\n",
    "        print (\"\\n\\n-------------------LOCAL LLM RESPONSE-------------------\")\n",
    "        start = time.time()\n",
    "        response_local = ask_question_local_llm(user_prompt)\n",
    "        end = time.time()   \n",
    "        print(f\"Time taken by Local LLM: {end - start} seconds\")\n",
    "        print(\"\\nLocal LLM says:\", response_local)        \n",
    "\n",
    "        # add delay of 3 seconds\n",
    "        time.sleep(3)\n",
    "    else:\n",
    "        print(\"Exiting...\")\n",
    "        break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
